[["index.html", "Análisis de Componentes Principales paso a paso con R 1 Sobre este manual 1.1 Un poco acerca del autor 1.2 Paquetes utilizados para la escritura de este manual", " Análisis de Componentes Principales paso a paso con R Juan Pablo Carreón Hidalgo 2022-10-21 1 Sobre este manual Este documento es una guía de cómo realizar análisis de componentes principales (PCA, por sus siglas en inglés) con R. Incluye una pequeña introducción al álgebra lineal, se expone el caso más simple de PCA en dos dimensiones y posteriormente se generaliza a más de tres dimensiones. También se expone un ejemplo aplicado al área de metabolómica y como un pequeño extra se aborda cómo utilizar una Shiny App que nos permita realizar PCA de manera automática. Para seguir este manual sin problemas se necesita ser un usuario de R con cierta experiencia. Al menos recomiendo saber lo básico de este lenguaje de programación, lo que sin duda incluye saber definir objetos y funciones, instalar y utilizar paquetes, así como la utilización de los paquetes incluidos en tidyverse. De forma opcional, también recomiendo utilizar R Studio, lo que facilita el trabajo con R de muchas formas. El primer capítulo de este manual aborda operaciones de álgebra lineal, por lo que recomiendo tener nociones de esta materia en un nivel básico. Al final se incluye un listado de referencias que pueden ser de utilidad si el lector o lectora no tiene mucha experiencia con lo que acabo de mencionar. En particular R para ciencia de datos es un excelente punto de partida para aprender a utilizar R y tidyverse. Cabe destacar que este manual hace énfasis en la utilización del PCA como una metodología para explorar datos, con ejemplos concretos en análisis de vinos, genómica y metabalómica de plantas. El PCA también puede utilizarse en Machine learning con el objetivo de reducir las dimensiones y peso de los datos y de esta forma acelerar el proceso en conjunto. Nada de esto último se aborda en este manual. Los capítulos en este manual son básicamente traducciones y adaptaciones de las publicaciones originales en mi blog R in the lab. Al igual que en mi blog, todo el contenido de este manual está bajo licencia Creative Commons Attribution 4.0 International License. Lo anterior implica que las personas que accedan a este manual pueden descargar, copiar, modificar y usar el texto, el código y las figuras con cualquier fin, siempre y cuando se haga la adecuada mención a la publicación original. El repositorio del libro con todo el código, texto y figuras para su creación se encuentran en el repositorio: Manual PCA. 1.1 Un poco acerca del autor Me gustaría resaltar que yo, el autor de este manual, no me especializo ni me dedico formalmente a la ciencia de datos o a la estadística, por lo que posiblemente el contenido de esta publicación no estará libre de omisiones o errores. Lo que sí puedo afirmar es que llevó unos seis años utilizando R y R Studio y a lo largo de todo este tiempo, he aprendido a utilizar diversas herramientas relacionadas con el análisis de datos, con la porgramación de Shiny Apps y la creación de blogs. También tengo un certificado profesional por HarvardX que puedes consultar en el siguiente enlace: Data Analysis for Life Sciences. Este es mi primer intento de realizar una publicación tipo manual con la ayuda del paquete bookdown. Me considero alguien autodidacta y sé bien que el conocimiento solo se puede asimilar realmente al compartir eso que acabas de aprender. Este manual y todas mis publicaciones son un intento por compartir y contribuir a mi comunidad. Donde quiera que estés y quienquiera que seas espero que todo lo abordado en cada capítulo te sea de utilidad y te contagié un poco de mi pasión por aprender. ¡Ojalá lea algo tuyo muy pronto, no dudes en compartirlo! Por favor, si detectas cualquier tipo de error o tienes cualquier tipo de duda sobre el contenido de este manual contáctame a través de mi correo electrónico (jpch_26@outlook.com) o facebook (https://www.facebook.com/jpch26). También puedo echarte una mano con tu aprendizaje de R y R Studio a un nivel básico . Ya por último, si está dentro de tus posibilidades, considera invitarme un cafecito a través de mi página de ko-fi ☕: Ko-fi de JPCH. ¡Muchas gracias y hasta la próxima! 1.2 Paquetes utilizados para la escritura de este manual Para la creación de este manual se utilizaron los paquetes listados a continuación, también se incluye la información de mi sistema operativo y otros datos. ## ─ Session info ─────────────────────────────────────────────────────────────── ## setting value ## version R version 4.2.1 (2022-06-23 ucrt) ## os Windows 10 x64 (build 19043) ## system x86_64, mingw32 ## ui RTerm ## language (EN) ## collate Spanish_Mexico.utf8 ## ctype Spanish_Mexico.utf8 ## tz America/Mexico_City ## date 2022-10-21 ## pandoc 2.18 @ C:/Program Files/RStudio/bin/quarto/bin/tools/ (via rmarkdown) ## ## ─ Packages ─────────────────────────────────────────────────────────────────── ## package * version date (UTC) lib source ## abind 1.4-5 2016-07-21 [1] CRAN (R 4.2.0) ## askpass 1.1 2019-01-13 [1] CRAN (R 4.2.1) ## assertthat 0.2.1 2019-03-21 [1] CRAN (R 4.2.1) ## backports 1.4.1 2021-12-13 [1] CRAN (R 4.2.0) ## base64enc 0.1-3 2015-07-28 [1] CRAN (R 4.2.0) ## bit 4.0.4 2020-08-04 [1] CRAN (R 4.2.1) ## bit64 4.0.5 2020-08-30 [1] CRAN (R 4.2.1) ## blob 1.2.3 2022-04-10 [1] CRAN (R 4.2.1) ## boot 1.3-28 2021-05-03 [2] CRAN (R 4.2.1) ## brio 1.1.3 2021-11-30 [1] CRAN (R 4.2.1) ## broom 1.0.0 2022-07-01 [1] CRAN (R 4.2.1) ## bslib 0.4.0 2022-07-16 [1] CRAN (R 4.2.1) ## cachem 1.0.6 2021-08-19 [1] CRAN (R 4.2.1) ## callr 3.7.1 2022-07-13 [1] CRAN (R 4.2.1) ## car 3.1-0 2022-06-15 [1] CRAN (R 4.2.1) ## carData 3.0-5 2022-01-06 [1] CRAN (R 4.2.1) ## cellranger 1.1.0 2016-07-27 [1] CRAN (R 4.2.1) ## cli 3.3.0 2022-04-25 [1] CRAN (R 4.2.1) ## clipr 0.8.0 2022-02-22 [1] CRAN (R 4.2.1) ## colorspace 2.0-3 2022-02-21 [1] CRAN (R 4.2.1) ## commonmark 1.8.0 2022-03-09 [1] CRAN (R 4.2.1) ## corrplot 0.92 2021-11-18 [1] CRAN (R 4.2.1) ## cowplot 1.1.1 2020-12-30 [1] CRAN (R 4.2.1) ## cpp11 0.4.2 2021-11-30 [1] CRAN (R 4.2.1) ## crayon 1.5.1 2022-03-26 [1] CRAN (R 4.2.1) ## crosstalk 1.2.0 2021-11-04 [1] CRAN (R 4.2.1) ## curl 4.3.2 2021-06-23 [1] CRAN (R 4.2.1) ## data.table 1.14.2 2021-09-27 [1] CRAN (R 4.2.1) ## DBI 1.1.3 2022-06-18 [1] CRAN (R 4.2.1) ## dbplyr 2.2.1 2022-06-27 [1] CRAN (R 4.2.1) ## desc 1.4.1 2022-03-06 [1] CRAN (R 4.2.1) ## diffobj 0.3.5 2021-10-05 [1] CRAN (R 4.2.1) ## digest 0.6.29 2021-12-01 [1] CRAN (R 4.2.1) ## dplyr 1.0.9 2022-04-28 [1] CRAN (R 4.2.1) ## dtplyr 1.2.1 2022-01-19 [1] CRAN (R 4.2.1) ## ellipsis 0.3.2 2021-04-29 [1] CRAN (R 4.2.1) ## evaluate 0.15 2022-02-18 [1] CRAN (R 4.2.1) ## fansi 1.0.3 2022-03-24 [1] CRAN (R 4.2.1) ## farver 2.1.1 2022-07-06 [1] CRAN (R 4.2.1) ## fastmap 1.1.0 2021-01-25 [1] CRAN (R 4.2.1) ## fontawesome 0.3.0 2022-07-20 [1] CRAN (R 4.2.1) ## forcats 0.5.1 2021-01-27 [1] CRAN (R 4.2.1) ## foreign 0.8-82 2022-01-16 [2] CRAN (R 4.2.1) ## fs 1.5.2 2021-12-08 [1] CRAN (R 4.2.1) ## gargle 1.2.0 2021-07-02 [1] CRAN (R 4.2.1) ## generics 0.1.3 2022-07-05 [1] CRAN (R 4.2.1) ## ggplot2 3.3.6 2022-05-03 [1] CRAN (R 4.2.1) ## ggpubr 0.4.0 2020-06-27 [1] CRAN (R 4.2.1) ## ggrepel 0.9.1 2021-01-15 [1] CRAN (R 4.2.1) ## ggsci 2.9 2018-05-14 [1] CRAN (R 4.2.1) ## ggsignif 0.6.3 2021-09-09 [1] CRAN (R 4.2.1) ## glue 1.6.2 2022-02-24 [1] CRAN (R 4.2.1) ## googledrive 2.0.0 2021-07-08 [1] CRAN (R 4.2.1) ## googlesheets4 1.0.0 2021-07-21 [1] CRAN (R 4.2.1) ## gridExtra 2.3 2017-09-09 [1] CRAN (R 4.2.1) ## gtable 0.3.0 2019-03-25 [1] CRAN (R 4.2.1) ## haven 2.5.0 2022-04-15 [1] CRAN (R 4.2.1) ## highr 0.9 2021-04-16 [1] CRAN (R 4.2.1) ## hms 1.1.1 2021-09-26 [1] CRAN (R 4.2.1) ## htmltools 0.5.3 2022-07-18 [1] CRAN (R 4.2.1) ## htmlwidgets 1.5.4 2021-09-08 [1] CRAN (R 4.2.1) ## httpuv 1.6.5 2022-01-05 [1] CRAN (R 4.2.1) ## httr 1.4.3 2022-05-04 [1] CRAN (R 4.2.1) ## ids 1.0.1 2017-05-31 [1] CRAN (R 4.2.1) ## isoband 0.2.5 2021-07-13 [1] CRAN (R 4.2.1) ## jquerylib 0.1.4 2021-04-26 [1] CRAN (R 4.2.1) ## jsonlite 1.8.0 2022-02-22 [1] CRAN (R 4.2.1) ## knitr 1.39 2022-04-26 [1] CRAN (R 4.2.1) ## labeling 0.4.2 2020-10-20 [1] CRAN (R 4.2.0) ## later 1.3.0 2021-08-18 [1] CRAN (R 4.2.1) ## lattice 0.20-45 2021-09-22 [2] CRAN (R 4.2.1) ## lazyeval 0.2.2 2019-03-15 [1] CRAN (R 4.2.1) ## lifecycle 1.0.1 2021-09-24 [1] CRAN (R 4.2.1) ## lme4 1.1-30 2022-07-08 [1] CRAN (R 4.2.1) ## lubridate 1.8.0 2021-10-07 [1] CRAN (R 4.2.1) ## magrittr 2.0.3 2022-03-30 [1] CRAN (R 4.2.1) ## maptools 1.1-4 2022-04-17 [1] CRAN (R 4.2.1) ## MASS 7.3-57 2022-04-22 [2] CRAN (R 4.2.1) ## Matrix 1.5-1 2022-09-13 [1] CRAN (R 4.2.1) ## MatrixModels 0.5-1 2022-09-11 [1] CRAN (R 4.2.1) ## memoise 2.0.1 2021-11-26 [1] CRAN (R 4.2.1) ## mgcv 1.8-40 2022-03-29 [2] CRAN (R 4.2.1) ## mime 0.12 2021-09-28 [1] CRAN (R 4.2.0) ## minqa 1.2.4 2014-10-09 [1] CRAN (R 4.2.1) ## modelr 0.1.8 2020-05-19 [1] CRAN (R 4.2.1) ## munsell 0.5.0 2018-06-12 [1] CRAN (R 4.2.1) ## nlme 3.1-157 2022-03-25 [2] CRAN (R 4.2.1) ## nloptr 2.0.3 2022-05-26 [1] CRAN (R 4.2.1) ## nnet 7.3-17 2022-01-16 [2] CRAN (R 4.2.1) ## numDeriv 2016.8-1.1 2019-06-06 [1] CRAN (R 4.2.0) ## openssl 2.0.2 2022-05-24 [1] CRAN (R 4.2.1) ## pbkrtest 0.5.1 2021-03-09 [1] CRAN (R 4.2.1) ## pillar 1.8.0 2022-07-18 [1] CRAN (R 4.2.1) ## pkgconfig 2.0.3 2019-09-22 [1] CRAN (R 4.2.1) ## pkgload 1.3.0 2022-06-27 [1] CRAN (R 4.2.1) ## plotly 4.10.0 2021-10-09 [1] CRAN (R 4.2.1) ## polynom 1.4-1 2022-04-11 [1] CRAN (R 4.2.1) ## praise 1.0.0 2015-08-11 [1] CRAN (R 4.2.1) ## prettyunits 1.1.1 2020-01-24 [1] CRAN (R 4.2.1) ## processx 3.7.0 2022-07-07 [1] CRAN (R 4.2.1) ## progress 1.2.2 2019-05-16 [1] CRAN (R 4.2.1) ## promises 1.2.0.1 2021-02-11 [1] CRAN (R 4.2.1) ## ps 1.7.1 2022-06-18 [1] CRAN (R 4.2.1) ## purrr 0.3.4 2020-04-17 [1] CRAN (R 4.2.1) ## quantreg 5.94 2022-07-20 [1] CRAN (R 4.2.1) ## R6 2.5.1 2021-08-19 [1] CRAN (R 4.2.1) ## rappdirs 0.3.3 2021-01-31 [1] CRAN (R 4.2.1) ## RColorBrewer 1.1-3 2022-04-03 [1] CRAN (R 4.2.0) ## Rcpp 1.0.9 2022-07-08 [1] CRAN (R 4.2.1) ## RcppEigen 0.3.3.9.2 2022-04-08 [1] CRAN (R 4.2.1) ## readr 2.1.2 2022-01-30 [1] CRAN (R 4.2.1) ## readxl 1.4.0 2022-03-28 [1] CRAN (R 4.2.1) ## rematch 1.0.1 2016-04-21 [1] CRAN (R 4.2.1) ## rematch2 2.1.2 2020-05-01 [1] CRAN (R 4.2.1) ## reprex 2.0.1 2021-08-05 [1] CRAN (R 4.2.1) ## rlang 1.0.4 2022-07-12 [1] CRAN (R 4.2.1) ## rmarkdown 2.14 2022-04-25 [1] CRAN (R 4.2.1) ## rprojroot 2.0.3 2022-04-02 [1] CRAN (R 4.2.1) ## rstatix 0.7.0 2021-02-13 [1] CRAN (R 4.2.1) ## rstudioapi 0.13 2020-11-12 [1] CRAN (R 4.2.1) ## rvest 1.0.2 2021-10-16 [1] CRAN (R 4.2.1) ## sass 0.4.2 2022-07-16 [1] CRAN (R 4.2.1) ## scales 1.2.0 2022-04-13 [1] CRAN (R 4.2.1) ## selectr 0.4-2 2019-11-20 [1] CRAN (R 4.2.1) ## shiny 1.7.2 2022-07-19 [1] CRAN (R 4.2.1) ## sourcetools 0.1.7 2018-04-25 [1] CRAN (R 4.2.1) ## sp 1.5-0 2022-06-05 [1] CRAN (R 4.2.1) ## SparseM 1.81 2021-02-18 [1] CRAN (R 4.2.0) ## stringi 1.7.8 2022-07-11 [1] CRAN (R 4.2.1) ## stringr 1.4.0 2019-02-10 [1] CRAN (R 4.2.1) ## survival 3.3-1 2022-03-03 [2] CRAN (R 4.2.1) ## sys 3.4 2020-07-23 [1] CRAN (R 4.2.1) ## testthat 3.1.5 2022-10-08 [1] CRAN (R 4.2.1) ## tibble 3.1.8 2022-07-22 [1] CRAN (R 4.2.1) ## tidyr 1.2.0 2022-02-01 [1] CRAN (R 4.2.1) ## tidyselect 1.1.2 2022-02-21 [1] CRAN (R 4.2.1) ## tidyverse 1.3.2 2022-07-18 [1] CRAN (R 4.2.1) ## tinytex 0.40 2022-06-15 [1] CRAN (R 4.2.1) ## tzdb 0.3.0 2022-03-28 [1] CRAN (R 4.2.1) ## utf8 1.2.2 2021-07-24 [1] CRAN (R 4.2.1) ## uuid 1.1-0 2022-04-19 [1] CRAN (R 4.2.0) ## vctrs 0.4.1 2022-04-13 [1] CRAN (R 4.2.1) ## viridisLite 0.4.0 2021-04-13 [1] CRAN (R 4.2.1) ## vroom 1.5.7 2021-11-30 [1] CRAN (R 4.2.1) ## waldo 0.4.0 2022-03-16 [1] CRAN (R 4.2.1) ## withr 2.5.0 2022-03-03 [1] CRAN (R 4.2.1) ## xfun 0.31 2022-05-10 [1] CRAN (R 4.2.1) ## xml2 1.3.3 2021-11-30 [1] CRAN (R 4.2.1) ## xtable 1.8-4 2019-04-21 [1] CRAN (R 4.2.1) ## yaml 2.3.5 2022-02-21 [1] CRAN (R 4.2.1) ## ## [1] C:/Users/Ciencia Libre/AppData/Local/R/win-library/4.2 ## [2] C:/Users/Ciencia Libre/AppData/Local/Programs/R/R-4.2.1/library ## ## ────────────────────────────────────────────────────────────────────────────── "],["algunas-operaciones-básicas-de-álgebra-lineal.html", " 2 Algunas operaciones básicas de álgebra lineal 2.1 Operaciones con vectores 2.2 Operaciones con matrices 2.3 Algunos extra", " 2 Algunas operaciones básicas de álgebra lineal En este capítulo veremos algunas operaciones de álgebra lineal. Esto será suficiente para posteriormente abordar el análisis de componentes principales desde sus bases. Si la momento de leer este capítulo no tienes mucha o ninguna experiencia con esta rama de las matemáticas, te recomiendo publicaciones como Nociones de geometría analítica y álgebra lineal o canales en YouTube como 3Blue1Brown (solo en inglés, pero con subtítulos en español). 2.1 Operaciones con vectores 2.1.1 Crear vectores En R podemos definir vectores numéricos de manera muy sencilla con la función c(). x &lt;- c(30, 20, 40, 10) y &lt;- c(20, 15, 18, 40) 2.1.2 Suma de vectores Para sumar un par de vectores utilizamos el operador + de la manera usual. Nota que cada elemento en el vector x se suma con el elemento en la misma posición que en el vector y. x + y ## [1] 50 35 58 50 En R, si intentamos sumar vectores con una longitud distinta, los elementos del vector más pequeño se reciclan para corresponder con el número de elementos del vector de mayor longitud. Esto se ve más claramente al realizar la operación en la consola de R. # Vector con dos elementos v2 &lt;- c(10, 40) x + v2 ## [1] 40 60 50 50 El vector v2 se utilizó dos veces de la forma c(10, 40, 10, 40) para corresponder con los elementos de x. Si el vector más largo tiene un número de elementos que no es un múltiplo del número de elementos del vector más pequeño, R desplegará un mensaje de advertencia. # Vector con tres elementos v3 &lt;- c(10, 40, 15) x + v3 ## Warning in x + v3: longitud de objeto mayor no es múltiplo de la longitud de uno ## menor ## [1] 40 60 55 20 Esto es un recordatorio de que al intentar reutilizar los elementos del vector más pequeño, algunos serán omitidos. En el ejemplo anterior, los elementos de v3 se utilizaron de la forma c(10, 40, 15, 10) para corresponder con la longitud de x. 2.1.3 Multiplicación de un vector por un escalar Para multiplicar un vector por un escalar utilizamos el operador *. 100 * x ## [1] 3000 2000 4000 1000 Aquí cada elemento en x se multiplicó por nuestro escalar 100. 2.1.4 Producto punto o producto interno Para obtener el producto interno o producto punto de dos vectores, utilizamos el operador %*%. z &lt;- x %*% y z ## [,1] ## [1,] 2020 Esta operación devuelve un objeto de las clases matriz y arreglo, o matrix y array en inglés. class(z) ## [1] &quot;matrix&quot; &quot;array&quot; Para obtener solo el valor numérico, utilizamos la función as.numeric(). as.numeric(z) ## [1] 2020 De manera un poco extendida o desglosada, al realizar el producto punto entre dos vectores cada elemento en el vector x se multiplica por el elemento en la misma posición que en el vector y y posteriormente cada producto se suma para obtener el total. En la consola de R lo anterior podría definirse de la siguiente forma. # Vectores &quot;x&quot; y &quot;y&quot; x &lt;- c(30, 20, 40, 10) y &lt;- c(20, 15, 18, 40) # Producto punto desglosado 30*20 + 20*15 + 40*18 + 10*40 ## [1] 2020 También debemos resaltar que los vectores deben tener el mismo número de elementos. De otra forma R nos mostrará un mensaje de error. x %*% v2 ## Error in x %*% v2: argumentos no compatibles 2.1.5 Norma o magnitud de un vector Para obtener la magnitud o norma de una vector podemos obtener la raíz cuadrada del producto punto del vector por sí mismo. sqrt(x %*% x) ## [,1] ## [1,] 54.77226 También podemos utilizar la función norm(), pero primero debemos asegurarnos que nuestro vector posea la clase matriz. x_m &lt;- as.matrix(x) norm(x_m, type = &quot;F&quot;) ## [1] 54.77226 2.2 Operaciones con matrices 2.2.1 Definir matrices Para definir una matriz en R usamos la función matrix() con un vector numérico como argumento. # Define un vector numérico v_n &lt;- c(7, -6, 12, 8) # Define la matriz m &lt;- matrix( v_n, nrow = 2, # Número de renglones en nuestra matriz byrow = TRUE, # Ordenar cada elemento por renglón ) m ## [,1] [,2] ## [1,] 7 -6 ## [2,] 12 8 Si el argumento byrow se establece como falso (FALSE), los elementos se ordenarán por columna. matrix(v_n, nrow = 2, byrow = FALSE) ## [,1] [,2] ## [1,] 7 12 ## [2,] -6 8 2.2.2 Multiplicar una matriz por un vector Para multiplicar una matriz por un vector utilizamos el operador %*%. Debemos cuidar que el número de elementos del vector sea igual al número de columnas en nuestra matriz. # Define una matriz m2 &lt;- matrix( c(8, 4, 5, 3, 1, 2), nrow = 3, byrow = TRUE ) m2 ## [,1] [,2] ## [1,] 8 4 ## [2,] 5 3 ## [3,] 1 2 # Define un vector z &lt;- c(3, 4) # Multiplica usando %*% m2 %*% z ## [,1] ## [1,] 40 ## [2,] 27 ## [3,] 11 Como puede verse, el resultado es un vector columna, que no es otra cosa que un objeto de la clase matriz con una sola columna. Al realizar este tipo de multiplicación debemos recordar que el orden del producto sí importa, tanto para la multiplicación de un vector por una matriz como para el producto entre matrices. Esto lo veremos con un poco de detalle más adelante. Para calcular cada elemento en el vector que obtuvimos en el ejemplo anterior, tomamos cada renglón de la matriz y realizamos el producto punto con el vector z. # Primer renglón de la matriz r1 &lt;- c(8, 4) # Segundo renglón r2 &lt;- c(5, 3) # Tercer renglón r3 &lt;- c(1, 2) # Elementos en el vector columna resultante cat(&quot;Primer elemento:&quot;, r1 %*% z, &quot;\\n&quot;) ## Primer elemento: 40 cat(&quot;Segundo elemento:&quot;, r2 %*% z, &quot;\\n&quot;) ## Segundo elemento: 27 cat(&quot;Tercer elemento:&quot;, r3 %*% z) ## Tercer elemento: 11 2.2.3 Multiplicación de matrices Para multiplicar dos matrices utilizamos el operador %*%. También debemos asegurarnos de que el número de columnas en la primera matriz sea igual número de renglones en la segunda. # Una matriz de dos renglones y tres columnas (2 X 3) A &lt;- matrix(c(1, 2, 3, 4, 0, 1), nrow = 2, byrow = TRUE) # Una matriz de tres renglones y tres columnas (3 X 3) B &lt;- matrix(c(1, 1, 0, 0, 1, 1, 1, 0, 1), nrow = 3, byrow = TRUE) # Producto de ambas matrices A %*% B ## [,1] [,2] [,3] ## [1,] 4 3 5 ## [2,] 5 4 1 De manera desglosada, cada elemento en la matriz resultante se calculó realizando el producto punto entre cada renglón de la matriz A por cada columna de la matriz B. Por ejemplo, calculemos el primer elemento, el número 4, en el resultado anterior. # Primer renglón de la matriz A ar_1 &lt;- c(1, 2, 3) # Primera columna de la matriz B bc_1 &lt;- c(1, 0, 1) # Producto punto entre ambos vectores ar_1 %*% bc_1 ## [,1] ## [1,] 4 Lo anterior se repite para obtener cada elemento de la matriz resultante. De manera general si la primera matriz tiene a renglones y b columnas (una matriz a X b) y la segunda matriz tiene b columnas y c renglones (una matriz b X c), la matriz que obtengamos de la multiplicación de ambas tendrá a renglones y c columnas (una matriz a X c). Pare terminar esta sección, retomemos lo que mencioné anteriormente, el orden de los factores en la multiplicación de matrices si altera el resultado. Esto se puede demostrar fácilmente definiendo un par de matrices de 2 X 2 (dos renglones y dos columnas). C &lt;- matrix(c(2, 4, 6, 0), nrow = 2) D &lt;- matrix(c(1, 3, 0, 9), nrow = 2) La multiplicación C %*% D: C %*% D ## [,1] [,2] ## [1,] 20 54 ## [2,] 4 0 La multiplicación D %*% C: D %*% C ## [,1] [,2] ## [1,] 2 6 ## [2,] 42 18 2.2.4 Multiplicación de un escalar por una matriz La multiplicación de un escalar por una matriz se hace de la misma forma que para vectores. 100 * A ## [,1] [,2] [,3] ## [1,] 100 200 300 ## [2,] 400 0 100 2.2.5 Transpuesta de una matriz Para obtener la transpuesta de una matriz usamos la función t(). Esta operación solo “intercambia” los renglones y columnas en la matriz original. Cada renglón pasa a ser una columna o viceversa, cada columna pasa a ser un renglón. Matriz original: A ## [,1] [,2] [,3] ## [1,] 1 2 3 ## [2,] 4 0 1 Matriz transpuesta: t(A) ## [,1] [,2] ## [1,] 1 4 ## [2,] 2 0 ## [3,] 3 1 2.2.6 Matriz identidad En ocasiones puede ser útil definir una matriz identidad o diagonal. Para esto utilizamos la función diag(). I &lt;- diag(nrow = 5) I ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 0 0 0 0 ## [2,] 0 1 0 0 0 ## [3,] 0 0 1 0 0 ## [4,] 0 0 0 1 0 ## [5,] 0 0 0 0 1 Como puede observarse, la matriz identidad tiene el mismo número de renglones y columnas y todos los elementos en la diagonal son 1. 2.2.7 Valores y vectores propios de una matriz La obtención de los valores y vectores propios de una matriz resulta una operación central al realizar análisis de componentes principales. De manera muy general, podemos pensar en una matriz como una función que puede modificar la dirección y magnitud de un vector o vectores dados. Por ejemplo, si definimos una matriz con dos columnas y dos renglones, esta podrá multiplicarse por vectores de dos elementos. Dichos vectores, a excepción del vector (0, 0), verán modificada su dirección y posiblemente su magnitud dependiendo de los elementos de la matriz. M &lt;- matrix(c(4, 0, 1, -1), nrow = 2, byrow = TRUE) v4 &lt;- c(3, 4) v4 %*% M ## [,1] [,2] ## [1,] 16 -4 En el ejemplo anterior, el vector resultante tiene coordenadas o elementos distintos, y si obtenemos la norma o magnitud de ambos vectores en este caso también resultarán diferentes. Ahora bien, un vector propio no verá modificada su dirección al multiplicarse por una matriz y en cuanto a su magnitud, esta podrá aumentar o disminuir por una constante o constantes dadas. Dichas constantes son los valores propios. Para decirlo de otra manera, los vectores propios son una especie de ejes que permanecen inmóviles al multiplicarse por un matriz y estos solo se estiran o contraen dependiendo de los valores propios. En R para obtener los vectores y valores propios de una matriz, utilizamos la función eigen(). Para nuestro ejemplo, definamos una matriz pequeña de dos renglones y dos columnas. A &lt;- matrix(c(3, 4, 0, 5), nrow = 2, byrow = TRUE) A ## [,1] [,2] ## [1,] 3 4 ## [2,] 0 5 Al utilizar la función eigen(), esta devolverá un listado con los vectores y valores propios. eA &lt;- eigen(A) Para acceder a los elementos de este listados utilizamos el operador $. Primero guardemos los valores propios en otro objeto. eA_vls &lt;- eA$values eA_vls ## [1] 5 3 Después hacemos lo mismo para los vectores. eA_vcs &lt;- eA$vectors eA_vcs ## [,1] [,2] ## [1,] 0.8944272 1 ## [2,] 0.4472136 0 En esta matriz cada columna es un vector propio. Si tomamos el segundo vector propio y los multiplicamos por el segundo valor propio, veremos que el vector apunta al mismo lugar, pero tendrá una longitud mayor. eA_vls[2] %*% eA_vcs[, 2] ## [,1] [,2] ## [1,] 3 0 2.3 Algunos extra Para terminar con este capítulo, a continuación muestro algunas procedimientos que pueden ser de utilidad para el lector. Claro, utilizando operaciones de álgebra lineal. 2.3.1 Resolver sistemas de ecuaciones lineales Digamos que queremos resolver el siguiente sistema de ecuaciones lineales con ayuda de R: \\(a + b + c = 15\\) \\(3a + 2b + c = 28\\) \\(2a + b + 2c = 23\\) En primer lugar, definimos una matriz con los coeficientes del sistema y un vector con los resultados o constantes de cada ecuación. En el caso de los coeficientes, cada renglón corresponderá a una ecuación y cada elemento corresponderá a un término, ya sea a, b o c. El orden de los coeficientes en cada renglón debe ser el mismo, en este caso se sigue el orden por abecedario. MC &lt;- matrix(c(1, 1, 1, 3, 2, 1, 2, 1, 2), nrow = 3, byrow = TRUE) MC ## [,1] [,2] [,3] ## [1,] 1 1 1 ## [2,] 3 2 1 ## [3,] 2 1 2 Para el vector de constantes también seguimos el orden por ecuación. vc &lt;- c(15, 28, 23) vc ## [1] 15 28 23 El siguiente paso es obtener la inversa de la matriz de coeficientes. Para esto usamos a la función solve(). MC_inv &lt;- solve(MC) MC_inv ## [,1] [,2] [,3] ## [1,] -1.5 0.5 0.5 ## [2,] 2.0 0.0 -1.0 ## [3,] 0.5 -0.5 0.5 Y finalmente, para obtener la solución, multiplicamos la inversa por el vector de coeficientes. sv &lt;- MC_inv %*% vc sv ## [,1] ## [1,] 3 ## [2,] 7 ## [3,] 5 Entonces a = 3, b = 7 y c = 5. Para verificar nuestra solución, podemos multiplicar sv por la matriz de coeficientes original (MC), lo que dará como resultado al vector de constantes (vc). MC %*% sv ## [,1] ## [1,] 15 ## [2,] 28 ## [3,] 23 2.3.2 Promedio y varianza con operaciones de álgebra lineal El álgebra lineal es fundamental y es utilizada para simplificar operaciones y reducir tiempos de computación. Muchas funciones y procedimientos estadísticos tienen en su centro operaciones de álgebra lineal. 2.3.2.1 Promedio Para calcular el promedio, primero simulemos un conjunto de 100 datos. La función set.seed() se utiliza para asegurar que cada vez que ejecutemos el mismo ejemplo obtengamos el mismo conjunto de datos. set.seed(5) vd &lt;- runif(100, min = 10, max = 35) Para obtener el promedio, necesitamos obtener la longitud del vector anterior y definir un vector de la misma longitud que solo contenga números 1. # Longitud (número de elementos) del vector vd ld &lt;- length(vd) # Vector con números 1 v1 &lt;- rep(1, ld) Multiplicamos el vector con nuestros datos por el vector de números 1, lo que resultará en la suma de todos los datos, y dividimos por la longitud del vector. vd_promedio &lt;- v1 %*% vd / ld vd_promedio ## [,1] ## [1,] 22.96092 Si utilizamos la función mean() con el vector vd como argumento obtendremos el mismo resultado. mean(vd) ## [1] 22.96092 2.3.3 Varianza Para calcular la varianza del mismo conjunto de datos, primero obtenemos la diferencia entre cada dato y el promedio. d &lt;- vd - as.numeric(vd_promedio) Multiplicamos el vector de diferencias por sí mismo, lo que dará como resultado la suma de las diferencias al cuadrado, y dividimos entre el número de datos menos uno (los grados de libertad). vd_varianza &lt;- d %*% d / ld - 1 vd_varianza ## [,1] ## [1,] 55.75781 Si utilizamos la función var() obtendremos el mismo resultado. var(vd) ## [1] 57.33112 "],["análisis-de-componentes-principales-en-dos-dimensiones.html", " 3 Análisis de componentes principales en dos dimensiones 3.1 Datos 3.2 Centrar los datos 3.3 Calcular la matriz de covarianza 3.4 Obtener los valores y vectores propios de la matriz de covarianza 3.5 Gráfica de barras para la varianza de cada componente principal 3.6 Pesos (Loading scores) 3.7 Representar los datos en menos dimensiones", " 3 Análisis de componentes principales en dos dimensiones Es hora de abordar el tema principal de este manual. De aquí en adelante abreviaré el nombre de “análisis de componentes principales” como PCA por sus siglas en inglés. El PCA es un análisis exploratorio de datos basado en la reducción de dimensiones. La idea general es reducir el conjunto de datos para que tenga menos dimensiones y, al mismo tiempo, conservar la mayor cantidad de información posible. El PCA nos permite hacer representaciones visuales en dos dimensiones y comprobar si hay grupos o diferencias en los datos relacionados con diferentes estados, tratamientos, etc. Además, podemos obtener alguna pista sobre qué variables en los datos son responsables de las diferencias visuales. Es importante destacar que el PCA no se utiliza exclusivamente para lo anterior y como es un análisis exploratorio las similitudes o diferencias de los datos deben ser consideradas en el contexto del que provienen. Comencemos con un caso simple que nos ayudará a comprender el PCA. Utilizaré solo dos variables para mostrar algunos conceptos básicos detrás y luego podremos generalizar a datos con más dimensiones. 3.1 Datos Para la primera variable tomé como base una distribución normal, y para la segunda consideré cierto grado de dependencia con la primera y añadí un poco de ruido también con una distribución normal. set.seed(1) # Variable 1 var_1 &lt;- rnorm(50, 50, sd = 3) # Variable 2 var_2 &lt;- .5*var_1 + rnorm(50, sd = sqrt(3)) # Ambas variables en un &quot;data.frame&quot; datos_1 &lt;- data.frame(var_1, var_2) head(datos_1) ## var_1 var_2 ## 1 48.12064 24.74986 ## 2 50.55093 24.21540 ## 3 47.49311 24.33739 ## 4 54.78584 25.43681 ## 5 50.98852 27.97633 ## 6 47.53859 27.19945 Un gráfico de dispersión puede mostrar la dispersión y la posible relación entre ambas variables: library(ggplot2) ggplot(datos_1, aes(x = var_1, y = var_2)) + geom_point(color = &quot;blue&quot;, size = 2) + xlab(&quot;Variable 1&quot;) + ylab(&quot;Variable 2&quot;) + theme_classic() 3.2 Centrar los datos El primer paso del PCA es centrar las dos variables respecto a su media. library(dplyr) datos_1 &lt;- datos_1 %&gt;% mutate(varc_1 = var_1 - mean(var_1), varc_2 = var_2 - mean(var_2)) head(datos_1) ## var_1 var_2 varc_1 varc_2 ## 1 48.12064 24.74986 -2.1807063 -0.60402890 ## 2 50.55093 24.21540 0.2495851 -1.13848362 ## 3 47.49311 24.33739 -2.8082307 -1.01649408 ## 4 54.78584 25.43681 4.4844976 0.08291914 ## 5 50.98852 27.97633 0.6871785 2.62244372 ## 6 47.53859 27.19945 -2.7627500 1.84556287 Lo anterior no modifica la posición relativa entre cada punto, por lo que los datos centrados tienen el mismo aspecto. ggplot(datos_1, aes(x = varc_1, y = varc_2)) + geom_point(color = &quot;blue&quot;, size = 2) + geom_vline(xintercept = 0, size = .5) + geom_hline(yintercept = 0, size = .5) + xlab(&quot;Variable 1 (valores centrados)&quot;) + ylab(&quot;Variable 2 (valores centrados)&quot;) + theme_classic() 3.3 Calcular la matriz de covarianza Se puede calcular la matriz de covarianza para un conjunto dado de variables simplemente realizando una multiplicación de matrices en los datos centrados. # Seleccionar las variables centradas datos_2 &lt;- datos_1 %&gt;% select(varc_1, varc_2) %&gt;% as.matrix() # Calcular la matriz de covarianza cov_m &lt;- (t(datos_2) %*% datos_2) / (nrow(datos_2) - 1) cov_m ## varc_1 varc_2 ## varc_1 6.220943 2.946877 ## varc_2 2.946877 4.207523 En esta matriz, la diagonal contiene las varianzas de cada variable, mientras que los valores fuera de la diagonal son las covarianzas entre ellas (véase la siguiente figura). El mismo resultado se puede obtener con la función cov(). cov(datos_2) ## varc_1 varc_2 ## varc_1 6.220943 2.946877 ## varc_2 2.946877 4.207523 O también con la función crossprod() de la siguiente forma. crossprod(datos_2) / (nrow(datos_2) - 1) ## varc_1 varc_2 ## varc_1 6.220943 2.946877 ## varc_2 2.946877 4.207523 3.4 Obtener los valores y vectores propios de la matriz de covarianza Los componentes principales son “líneas” o direcciones que capturan la mayor parte de la información en los datos. Estas direcciones pueden obtenerse calculando los valores y vectores propios de la matriz de covarianza. # Función eigen() para obtener los vectores y valores propios cov_e &lt;- eigen(cov_m) # Vectores propios e_vec &lt;- cov_e$vectors # Valores propios e_val &lt;- cov_e$values La extensión de cada vector propio (span en inglés) puede considerarse la “línea” que capta la mayor parte de la variación. # Primer vector propio ev_1 &lt;- e_vec[,1] # Pendiente del primer vector propio ev1_m &lt;- ev_1[2] / ev_1[1] # Segundo vector propio ev_2 &lt;- e_vec[,2] # Pendiente del segundo vector propio ev2_m &lt;- ev_2[2] / ev_2[1] # Gráfica de dispersión de los datos centrados mostrando la extensión de # cada vector propio ggplot(data.frame(datos_2), aes(x = varc_1, y = varc_2)) + geom_point(color = &quot;blue&quot;, size = 2) + geom_vline(xintercept = 0, size = .5) + geom_hline(yintercept = 0, size = .5) + geom_abline(slope = ev1_m, color = &quot;blue&quot;, size = 0.7) + geom_abline(slope = ev2_m, color = &quot;red&quot;, size = 0.7) + xlab(&quot;Variable 1 (valores centrados)&quot;) + ylab(&quot;Variable 2 (valores centrados)&quot;) + theme_classic() 3.5 Gráfica de barras para la varianza de cada componente principal Al dividir cada valor propio por n - 1, donde n es el número de renglones en los datos originales, obtendremos una estimación de la varianza de cada componente principal. La suma de todas las varianzas (la varianza total) puede utilizarse para calcular el porcentaje de variación de cada componente y, finalmente, podemos visualizar los porcentajes con una gráfica de barras (en inglés a esta gráfica se le conoce como Scree plot). # Calcular la varianza estimada para cada valor propio e_var &lt;- e_val / (nrow(datos_2) - 1) # &quot;Data frame&quot; con los porcentajes de variación var_per &lt;- data.frame( PC = c(&quot;PC1&quot;, &quot;PC2&quot;), PER = c(e_var) * 100 / sum(e_var) # Calcular los porcentajes ) # &quot;Scree plot&quot; ggplot(var_per, aes(x = PC, y = PER)) + geom_col(width = 0.5, color = &quot;black&quot;) + xlab(&quot;Componente principal&quot;) + ylab(&quot;Porcentaje de variación (%)&quot;) + theme_classic() 3.6 Pesos (Loading scores) Los vectores propios obtenidos mediante la función eigen() están normalizados. Esto quiere decir que su longitud es igual a 1. # Norma del primer vector propio norm(as.matrix(ev_1), &quot;F&quot;) ## [1] 1 # Norma del segundo vector propio norm(as.matrix(ev_2), &quot;F&quot;) ## [1] 1 Los elementos de cada vector propio también se denominan pesos (loadings en inglés) y pueden interpretarse como la contribución de cada variable en los datos originales a un componente principal en específico, o, más estrictamente, se pueden interpretar como los coeficientes de la combinación lineal de las variables originales a partir de las cuales se construyen los componentes principales. Puedes realizar una tabla con estos valores y observar las contribuciones de cada variable a cada componente principal. # &quot;Data frame&quot; con ambos valores propios loads &lt;- data.frame( VAR = c(&quot;var_1&quot;, &quot;var_2&quot;), PC1 = ev_1, # Primer vector propio PC2 = ev_2 # Segundo vector propio ) loads ## VAR PC1 PC2 ## 1 var_1 -0.8134113 0.5816890 ## 2 var_2 -0.5816890 -0.8134113 Lo anterior puede ser útil en datos con muchas dimensiones para establecer qué variables son las causantes de las agrupaciones o diferencias en las gráficas de dispersión del PCA. 3.7 Representar los datos en menos dimensiones Cambiar la base de los datos originales por la indicada por los vectores propios, producirá una “rotación” de los datos. # Inversa de la matriz con los vectores propios inv_evec &lt;- solve(e_vec) # Cambio de base de los datos originales datos_3 &lt;- datos_2 %*% inv_evec # Gráfica de dispersión donce se muestra la dispersión ggplot(data.frame(datos_3), aes(X1, X2)) + geom_point(color = &quot;blue&quot;, size = 2) + geom_vline(xintercept = 0, size = .5) + geom_hline(yintercept = 0, size = .5) + xlab(&quot;PC1 (78.8%)&quot;) + ylab(&quot;PC2 (21.2%)&quot;) + theme_classic() Podemos comparar la gráfica anterior con la de los datos originales para tener una idea más clara de como se rotaron los datos una vez que cambiamos su base. library(ggpubr) # Gráfica de dispersión con los datos centrados dg &lt;- ggplot(data.frame(datos_2), aes(varc_1, varc_2)) + geom_point(color = &quot;blue&quot;, size = 2) + geom_vline(xintercept = 0, size = .5) + geom_hline(yintercept = 0, size = .5) + ylim(c(-8, 8.5)) + ggtitle(&quot;Datos originales (Centrados)&quot;) + theme_classic() # Gráfica de dispersión con los datos rotados dr &lt;- ggplot(data.frame(datos_3), aes(X1, X2)) + geom_point(color = &quot;blue&quot;, size = 2) + geom_vline(xintercept = 0, size = .5) + geom_hline(yintercept = 0, size = .5) + xlab(&quot;PC1 (78.8%)&quot;) + ylab(&quot;PC2 (21.2%)&quot;) + ylim(c(-8, 8.5)) + ggtitle(&quot;Cambio de base&quot;) + theme_classic() # Ambas gráficas lado a lado ggarrange(dg, dr) Ya que el componente principal 1 (PC1) explica la mayor parte de la variación en los datos, podemos omitir el componente principal 2 (PC2) y representar cada punto en una sola dimensión. A continuación represento los datos en una sola dimensión con puntos rojos. # Datos del componente principal 1 datos_pc1 &lt;- data.frame(v1 = datos_3[,1], v2 = rep(0, nrow(datos_3))) # Gráfica de dispersión mostrando los datos de PC1 ggplot(data.frame(datos_3), aes(X1, X2)) + geom_point(color = &quot;blue&quot;, size = 2) + geom_point(data = datos_pc1, aes(v1, v2), color = &quot;red&quot;, size = 2) + geom_vline(xintercept = 0, size = .5) + geom_hline(yintercept = 0, size = .5) + xlab(&quot;PC1 (78.8%)&quot;) + ylab(&quot;PC2 (21.2%)&quot;) + ylim(c(-8, 8.5)) + theme_classic() Las ideas anteriores pueden utilizarse en datos con muchas variables para reducir las dimensiones y representar los datos con gráficos de dispersión en dos dimensiones. "],["genaralización-del-pca-a-más-de-tres-dimensiones.html", " 4 Genaralización del PCA a más de tres dimensiones 4.1 Datos 4.2 Centrar cada variable y dividir por su desviación estándar 4.3 Calcular la matriz de covarianzas 4.4 Obtener los valores y vectores propios de la matriz de covarianzas 4.5 Gráfica de barras para el porcentaje de variación de cada componente principal 4.6 Representar los datos en dos dimensiones 4.7 Pesos (Loading scores)", " 4 Genaralización del PCA a más de tres dimensiones En este capítulo usaremos datos reales en muestras de vino provenientes de distintos países. Todos los datos pueden encontrarse en el repositorio de este libro en la carpeta “datos”. 4.1 Datos En las muestras de vino se midieron las siguientes variables o parámetros de calidad. Las muestras de vino procedieron de Argentina, Chile, Australia y Sudáfrica. Las seis primeras filas renglores del conjunto de datos tienen este aspecto. # Nombres de las variables o parámetro de calidad nombres_var &lt;- read.csv(&quot;datos/Label_Pred_values_IR.csv&quot;) nombres_var &lt;- names(nombres_var) # Etiquetas o códigos en las muestras de vino cod_vino &lt;- read.csv(&quot;datos/Label_Wine_samples.csv&quot;, header = FALSE) cod_vino &lt;- unname(unlist(cod_vino)) # Conjunto de datos completo para las muestras de vino datos_vino &lt;- read.csv( &quot;datos/Pred_values.csv&quot;, header = FALSE, row.names = cod_vino, col.names = nombres_var ) head(datos_vino) ## Etanol Acidez AcVolatiles AcMalico pH AcLactico AzucarReductor ## ARG-BNS1 13.62 3.54 0.29 0.89 3.71 0.78 1.46 ## ARG-DDA1 14.06 3.74 0.59 0.24 3.73 1.25 2.42 ## ARG-FFL1 13.74 3.27 0.47 -0.07 3.87 1.13 1.52 ## ARG-FLM1 13.95 3.66 0.47 0.09 3.79 1.00 4.17 ## ARG-ICR1 14.47 3.66 0.38 0.61 3.70 0.81 1.25 ## ARG-SAL1 14.61 3.45 0.52 0.16 3.92 1.76 1.40 ## AcCitrico CO2 Densidad Polifenoles Glicerol Metanol AcTartarico ## ARG-BNS1 0.31 85.61 0.99 60.92 9.72 0.16 1.74 ## ARG-DDA1 0.18 175.20 1.00 70.64 10.05 0.20 1.58 ## ARG-FFL1 0.39 513.74 0.99 63.59 10.92 0.18 1.24 ## ARG-FLM1 0.41 379.40 1.00 73.30 9.69 0.23 2.26 ## ARG-ICR1 0.14 154.88 0.99 71.69 10.81 0.20 1.22 ## ARG-SAL1 0.10 156.30 0.99 71.79 10.19 0.19 0.90 En este conjunto de datos las muestras de vino están marcadas como los nombres de las filas o renglones, y cada muestra tiene asociados los valores de cada parámetro de calidad (las columnas). Es importante prestar atención a la información señalada en columnas y renglones al calcular las matrices de covarianza o utilizar funciones para realizar el PCA, ¿queremos reducir las dimensiones con respecto a las filas o con respecto a las columnas? En este ejemplo nos interesa reducir el número de dimensiones con respecto a los parámetros de calidad y tratar de detectar si hay algunas similitudes o diferencias entre las muestras de vino. 4.2 Centrar cada variable y dividir por su desviación estándar Primero restamos el promedio de cada variable (columnas) y dividimos el resultado entre la desviación estándar. library(purrr) # Promedios de cada variable prom_vars &lt;- unlist(map(datos_vino, mean)) # Desviación estándar de cada variable de_vars &lt;- unlist(map(datos_vino, sd)) # Centrar cada variable datos_vino_2 &lt;- map2( datos_vino, prom_vars, .f = function(x, mean) x - mean ) # Dividir por la desviación estándar de cada variable datos_vino_2 &lt;- map2( datos_vino_2, de_vars, .f = function(x, sd) x / sd ) # Generar una matriz a partir de la lista anterior datos_vino_2 &lt;- as.matrix(data.frame(datos_vino_2)) Cada renglón de los datos transformados corresponde a la misma muestra de vino que en los datos originales. Los primeros seis renglones de los datos transformados tienen el siguiente aspecto. head(datos_vino_2) ## Etanol Acidez AcVolatiles AcMalico pH AcLactico ## [1,] -0.63678490 -0.40870999 -0.7811641 2.2627949 0.18823560 -0.88226788 ## [2,] 0.29224039 0.01096889 1.3231961 -0.5694123 0.40060358 0.38388752 ## [3,] -0.38341492 -0.97527630 0.4814521 -1.9201573 1.88717937 0.06061381 ## [4,] 0.05998306 -0.15690246 0.4814521 -1.2229986 1.03770749 -0.28959935 ## [5,] 1.15792166 -0.15690246 -0.1498560 1.0427673 0.08205162 -0.80144937 ## [6,] 1.45351897 -0.59756526 0.8321787 -0.9179917 2.41810183 1.75780072 ## AzucarReductor AcCitrico CO2 Densidad Polifenoles Glicerol ## [1,] -0.5311188 1.1594920 -1.6049333 -0.7844233 -0.1371321 -0.525885112 ## [2,] 0.1996454 0.1084153 -0.9482843 1.2458487 1.2376582 -0.160707984 ## [3,] -0.4854461 1.8063082 1.5330409 -0.7844233 0.2405111 0.802031811 ## [4,] 1.5317676 1.9680124 0.5483973 1.2458487 1.6138874 -0.559083800 ## [5,] -0.6909735 -0.2149929 -1.0972195 -0.7844233 1.3861700 0.680306454 ## [6,] -0.5767916 -0.5384011 -1.0868116 -0.7844233 1.4003137 -0.005784994 ## Metanol AcTartarico ## [1,] -1.1081037 0.43877622 ## [2,] -0.0870238 -0.03865555 ## [3,] -0.5975635 -1.05319837 ## [4,] 0.6787860 1.99042974 ## [5,] -0.0870238 -1.11287729 ## [6,] -0.3422939 -2.06774119 Dividir por las desviaciones estándar es una forma de dar a cada variable la misma importancia a pesar de su rango, magnitud y/o escala de medición. Además de dividir por la desviación estándar, son posibles otras transformaciones que pueden aplicarse en función de los datos. Consulte los referencias al final de este manual si está interesado. 4.3 Calcular la matriz de covarianzas Para obtener la matriz de covarianzas multiplicamos los datos (como una matriz) por su traspuesta y dividimos por el número de renglones menos uno (los grados de libertad). # Calcular la matriz de covarianzas vino_cov &lt;- (t(datos_vino_2) %*% datos_vino_2) / (nrow(datos_vino_2) - 1) vino_cov[1:5, 1:5] ## Etanol Acidez AcVolatiles AcMalico pH ## Etanol 1.00000000 0.3262321 0.2028382 0.04166778 0.1697347 ## Acidez 0.32623209 1.0000000 0.4660575 -0.26067574 -0.3518303 ## AcVolatiles 0.20283816 0.4660575 1.0000000 -0.74628880 0.3011611 ## AcMalico 0.04166778 -0.2606757 -0.7462888 1.00000000 -0.2929542 ## pH 0.16973470 -0.3518303 0.3011611 -0.29295421 1.0000000 Solo se muestran los primeros cinco renglones y columnas de la matriz de covarianza. Si estás utilizando R Studio, puedes usar la función View() con vino_covcomo argumento para desplegar una ventana completa que mostrará los datos como en una hoja de Excel. En la matriz anterior, los valores en la diagonal son las varianzas de cada variable y los valores fuera de la diagonal son las covarianzas entre las variables. Como puede observarse, todas las varianzas son iguales a 1. Esto es precisamente el efecto de centrar y dividir por la desviación estándar de cada variable. 4.4 Obtener los valores y vectores propios de la matriz de covarianzas Para obtener los valores y vectores propios usamos la función eigen(). vino_eg &lt;- eigen(vino_cov) # Valores propios val_eg &lt;- vino_eg$values # Vectores propios vec_eg &lt;- vino_eg$vectors El número de vectores y valores propios es el mismo que el número de variables en el conjunto de datos originales. # Número de valores propios length(val_eg) ## [1] 14 # Número de vectores propios ncol(vec_eg) ## [1] 14 4.5 Gráfica de barras para el porcentaje de variación de cada componente principal Ahora calculamos el porcentaje de variación de cada componente y realizamos una gráfica de barras. # Calcular la varianza de cada valor propio vars_val_eg &lt;- val_eg / (nrow(datos_vino_2) - 1) # &quot;Data frame&quot; con los porcentajes de variación vars_perc &lt;- data.frame( PC = unlist(map(1:14, function(x) paste0(&quot;PC&quot;, x))), PER = round((vars_val_eg * 100) / sum(vars_val_eg), 4) ) # Gráfica de barras library(ggplot2) ggplot( vars_perc, aes(x = reorder(PC, order(PER, decreasing = TRUE)), y = PER) ) + geom_col(width = 0.5, color = &quot;black&quot;) + xlab(&quot;Componente Principal&quot;) + ylab(&quot;Porcentaje de variación (%)&quot;) + theme_classic() 4.6 Representar los datos en dos dimensiones Idealmente, si los primeros dos componentes reunieran la mayor parte de la variación, digamos más del 90%, sería posible hacer una buena representación de los datos en una gráfica de dispersión en dos dimensiones. Ya que la mayor parte del tiempo los datos reales rara vez son ideales, en este ejemplo utilizaremos PC1, PC2, PC3 y PC4, que en conjunto representan el 73% de la variación. En estos casos podemos tratar de detectar grupos haciendo dos gráficas de dispersión, el primero con PC1 y PC2, y el segundo con PC3 y PC4. En primer lugar, cambiamos la base de los datos transformados por la indicada por los vectores propios. library(dplyr) # Cambiar la base de los datos centrados datos_vino_cb &lt;- datos_vino_2 %*% solve(vec_eg) # Transformar a un &quot;data frame&quot; datos_vino_cb &lt;- data.frame(datos_vino_cb) colnames(datos_vino_cb) &lt;- vars_perc$PC # Añadir una columna con los datos de origen de cada muestra de vino datos_vino_cb &lt;- datos_vino_cb %&gt;% mutate( MuestraVino = unlist(map(cod_vino, function(x) substr(x, 1, 3))) ) %&gt;% relocate(MuestraVino) head(datos_vino_cb) ## MuestraVino PC1 PC2 PC3 PC4 PC5 ## 1 ARG -0.8340408 0.8560257 1.2477283 -0.2946354 -2.64962606 ## 2 ARG 0.9182495 -0.7520145 0.8955902 -0.3967229 0.06460487 ## 3 ARG 2.0109822 0.2126599 -0.4739304 -1.3446676 0.96328060 ## 4 ARG 2.8454813 -1.9323804 0.5129401 -0.2805325 -0.56882676 ## 5 ARG -0.4027013 0.7424389 0.3544802 -1.3925516 -0.86093254 ## 6 ARG 0.4921766 0.6976268 0.9133225 -1.3858705 0.58195738 ## PC6 PC7 PC8 PC9 PC10 PC11 ## 1 -0.09972511 0.6839127 -0.64470453 1.30296999 0.08192591 0.90864847 ## 2 -1.15201976 -0.6156587 0.09652472 -0.66508130 0.22293709 -0.44745146 ## 3 -0.97714541 0.7454403 1.13873701 -0.36070456 -1.80323468 1.57277920 ## 4 0.04685162 0.9391345 1.05427365 0.06617118 -0.72767986 -1.40159736 ## 5 -0.43249796 -1.2152728 0.19062688 1.02200106 0.80693595 -0.03070849 ## 6 -1.12762870 -0.7447312 2.12377275 -1.45181400 0.27437060 1.29332051 ## PC12 PC13 PC14 ## 1 0.09448821 0.2045349 0.2889872 ## 2 0.60237715 -1.1576675 -0.2344966 ## 3 -0.74179131 0.9287338 -0.2968222 ## 4 0.97819193 -0.1854973 0.5890199 ## 5 -0.89310589 -0.3806547 -1.1129547 ## 6 -1.49226046 -1.9552497 -1.2846205 Ahora hagamos ambas gráficas de dispersión tomando los valores de PC1, PC2, PC3 y PC4. library(ggpubr) # Gráfica de dispersión para PC1 y PC2 pc12 &lt;- ggplot( datos_vino_cb, aes(PC1, PC2, color = MuestraVino, shape = MuestraVino) ) + geom_point(size = 3) + ggtitle(&quot;PC1 y PC2&quot;) + xlab(&quot;PC1 (24.4%)&quot;) + ylab(&quot;PC2 (21.3%)&quot;) + theme_classic() + theme(legend.position = &quot;none&quot;) # Gráfica de dispersión para PC3 y PC4 pc34 &lt;- ggplot( datos_vino_cb, aes(PC3, PC4, color = MuestraVino, shape = MuestraVino) ) + geom_point(size = 3) + scale_color_discrete( name = &quot;País de origen&quot;, labels = c(&quot;Argentina&quot;, &quot;Australia&quot;, &quot;Chile&quot;, &quot;Sudáfrica&quot;) ) + scale_shape_discrete( name = &quot;País de origen&quot;, labels = c(&quot;Argentina&quot;, &quot;Australia&quot;, &quot;Chile&quot;, &quot;Sudáfrica&quot;) ) + ggtitle(&quot;PC3 y PC4&quot;) + xlab(&quot;PC3 (17.5%)&quot;) + ylab(&quot;PC4 (10.0%)&quot;) + theme_classic() # Ambas gráficas lado a lado ggarrange(pc12, pc34, widths = c(1.5, 2)) 4.7 Pesos (Loading scores) Los elementos de cada vector propio representan el peso de cada variable en el componente principal correspondiente. # &quot;Data frame&quot; con los pesos pesos_vino &lt;- data.frame(vec_eg) colnames(pesos_vino) &lt;- vars_perc$PC rownames(pesos_vino) &lt;- nombres_var head(pesos_vino) ## PC1 PC2 PC3 PC4 PC5 ## Etanol -0.2050720 -0.3452884 0.28833198 -0.33833697 0.001102612 ## Acidez -0.1457210 -0.4545803 -0.06374909 0.42139270 0.041508880 ## AcVolatiles 0.2959952 -0.4418538 0.03338975 0.08430388 -0.010891943 ## AcMalico -0.3401714 0.3173079 0.17907611 -0.04002812 -0.252870228 ## pH 0.2413635 -0.1030772 -0.04459655 -0.66472873 -0.170592676 ## AcLactico 0.3462627 -0.3745378 -0.16738658 0.11850141 -0.108643313 ## PC6 PC7 PC8 PC9 PC10 ## Etanol -0.02992588 0.106645513 0.40565761 0.2220668 -0.0736428 ## Acidez -0.10721872 0.109080109 -0.15801516 0.1647662 -0.1264076 ## AcVolatiles 0.12676353 -0.117134956 -0.01066998 -0.2323371 0.1969852 ## AcMalico 0.09495473 0.247290148 -0.35870967 0.1792700 -0.2720317 ## pH 0.01717167 -0.191605072 -0.30040129 0.3352702 0.2882746 ## AcLactico 0.05496474 0.008009813 -0.10169448 0.1377493 -0.2860564 ## PC11 PC12 PC13 PC14 ## Etanol 0.52247393 -0.22486081 0.29025391 0.04195694 ## Acidez -0.09209975 -0.25488777 -0.01537897 -0.65083548 ## AcVolatiles 0.19114512 -0.33800897 -0.59864003 0.27743084 ## AcMalico -0.07338526 -0.53559762 -0.18854505 0.23066288 ## pH -0.21200578 -0.09998927 -0.02838824 -0.28223548 ## AcLactico -0.32247769 -0.11583440 0.49548494 0.45696212 Hacer una gráfica de dispersión con estos valores puede ayudar a ver patrones de variación y/o explicar las agrupaciones o diferencias en las gráficas de dispersión de los componentes principales. # Dispersión con pesos de PC1 y PC2 ld_pc12 &lt;- ggplot(pesos_vino, aes(PC1, PC2)) + geom_point(color = &quot;blue&quot;, size = 2) + geom_vline(xintercept = 0) + geom_hline(yintercept = 0) + geom_text(aes(label = rownames(pesos_vino)), hjust = -.2) + ggtitle(&quot;Pesos para PC1 y PC2&quot;) + xlim(c(-.7, .7)) + ylim(c(-.7, .7)) + xlab(&quot;PC1 (24.4%)&quot;) + ylab(&quot;PC2 (21.3%)&quot;) + theme_classic() # Dispersión con pesos de PC3 y PC4 ld_pc34 &lt;- ggplot(pesos_vino, aes(PC3, PC4)) + geom_point(color = &quot;blue&quot;, size = 2) + geom_vline(xintercept = 0) + geom_hline(yintercept = 0) + geom_text(aes(label = rownames(pesos_vino)), hjust = -.2) + ggtitle(&quot;Pesos para PC3 y PC4&quot;) + xlim(c(-.7, .7)) + ylim(c(-.7, .7)) + xlab(&quot;PC3 (17.5%)&quot;) + ylab(&quot;PC4 (10.0%)&quot;) + theme_classic() # Ambas gráficas lado a lado ggarrange(ld_pc12, ld_pc34) "],["análisis-de-componentes-principales-con-descomposición-de-valores-singulares.html", " 5 Análisis de componentes principales con descomposición de valores singulares 5.1 Descomposición de valores singulares 5.2 SVD para llevar a cabo PCA 5.3 Datos 5.4 Análisis de componentes principales con la función svd() 5.5 Análisis de componente principales con la función prcomp()", " 5 Análisis de componentes principales con descomposición de valores singulares En este capítulo mostraré como llevar a cabo PCA con funciones específicas para esta tarea en R. Esto nos ayudará a simplificar los pasos cada vez que necesitemos llevar a cabo este análisis. 5.1 Descomposición de valores singulares La descomposición de valores singulares (SVD por sus siglas en inglés) es una factorización que generaliza la obtención de valores y vectores propios en matrices cuadradas (mismo número de renglones y columnas) a matrices con cualquier número de renglones y columnas: Donde M es una matriz de m renglones y n columnas (m x n), S es una matriz m x n y V es una matriz n x n. Algunos puntos importantes sobre el SVD: Los elementos en la diagonal de S se denominan valores singulares de la matriz M. El número de valores singulares distintos de cero es igual al rango de la matriz M. Las columnas de U y las columnas de V se denominan vectores singulares de M. 5.2 SVD para llevar a cabo PCA La SVD puede ser usada para realizar PCA en un conjunto de datos ya que existe una relación estrecha entre ambos procedimientos. Digamos que tenemos una matriz M de tamaño m x n. En los renglones m especificamos las categorías, estados, muestras, tratamientos, etc., y en las columnas n señalamos las distintas variables medidas. Supongamos también que hemos centrado los datos, mediante la resta del promedio de cada variable, y dividido entre las desviaciones estándar. Entonces podemos calcular la matriz de covarianzas de la siguiente forma: Esta matriz simétrica o cuadrada (mismo número de renglones y columnas) puede ser diagonalizada de la siguiente forma: En la ecuación anterior, V es la matriz de vectores propios (cada columna es un vector propio) y L es una matriz donde los elementos en la diagonal son los valores propios. Ahora bien, si llevamos a cabo SVD en M como en la primera ecuación: Podemos establecer que: Y finalmente la matriz de covarianza C se puede expresar como: Esto quiere decir que los vectores singulares en V son los vectores propios de la matriz y los valores singulares en S están relacionados con los valores propios mediante \\(ValorPropio=ValorSingular^2/(n-1)\\). Los componentes principales están dados mediante la relación entre matrices \\(XV = US\\) y los vectores propios son las columnas de la matriz \\(V\\). Ahora apliquemos directamente lo que acabamos de ver para llevar a cabo un PCA en datos reales. 5.3 Datos El conjunto de datos que usaremos en este capítulo provienen del repositorio tissuesGeneExpression. Estos son datos de expresión genómica cuantificados en distintos tejidos humanos. Primero descargamos dichos datos mediante el siguiente código. devtools::install_github(&quot;genomicsclass/tissuesGeneExpression&quot;) library(tissuesGeneExpression) data(tissuesGeneExpression) datos_genes &lt;- t(e) datos_genes &lt;- as.data.frame(datos_genes) En este caso usamos la función t() para obtener la transpuesta de los datos originales. De este forma los renglones corresponden a las muestras de tejido y las columnas a los distintos genes. dim(datos_genes) ## [1] 189 22215 En este conjunto de datos se tienen 189 muestras de diferentes tejidos y un total de 22 215 genes. 5.4 Análisis de componentes principales con la función svd() Para utilizar la función svd() para llevar a cabo PCA, primero centramos los datos restando la media de cada gen y dividimos entre las desviaciones estándar. library(purrr) # Promedio de cada gen prom_gen &lt;- map_dbl(datos_genes, mean) # Desviaciones estándar de cada gen de_gen &lt;- map_dbl(datos_genes, sd) # Centrar los datos y dividir entre la desviación estándar datos_genes_c &lt;- map2(datos_genes, prom_gen, .f = function(x, mean) x - mean) datos_genes_c &lt;- map2(datos_genes_c, de_gen, .f = function(x, sd) x/sd) datos_genes_c &lt;- data.frame(datos_genes_c) Ahora podemos realizar SVD sobre los datos transformados. dg_svd &lt;- svd(datos_genes_c) El objeto dg_svd es una lista que contiene a la matriz U, la matriz V y un vector numérico con los valores singulares. # Matriz U dg_u &lt;- dg_svd$u # Matriz V dg_v &lt;- dg_svd$v # Matriz con valores singulares dg_d &lt;- diag(dg_svd$d) 5.4.1 Gráfica de barras para la varianza de cada componente principal Ahora calculamos cada valor propio y el porcentaje de variación de cada componente principal. library(dplyr) ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union library(ggplot2) # Valores propios vp &lt;- dg_svd$d^2 / (nrow(datos_genes) - 1) # Porcentajes de variación de cada PC per_pc &lt;- data.frame(vp) %&gt;% mutate( per_var = vp * 100 / sum(vp), pc = map_chr(1:nrow(dg_u), .f = function(x) paste0(&quot;PC&quot;, as.character(x)) ) ) # Gráfica de barras con la varianza de cada PC ggplot(per_pc[1:15,], aes(reorder(pc, -per_var), per_var)) + geom_col() + xlab(&quot;Componente Principal&quot;) + ylab(&quot;Porcentaje de variación (%)&quot;) + theme_classic() Ya que hay 189 componentes principales y generalmente la variación está acumulada en unos cuantos de los primeros, en la gráfica anterior solo se tomaron los primeros 15 componentes. 5.4.2 Representación de los datos en dos dimensiones Primero calculamos las proyecciones de los datos originales. # Proyecciones dg_p &lt;- data.frame(dg_u %*% dg_d) # Cambiar los nombres de las columnas por los de los componente principales colnames(dg_p) &lt;- map_chr( 1:nrow(dg_p), .f = function(x) paste0(&quot;PC&quot;, as.character(x)) ) # Añadir una columna nueva con el nombre de los tejidos dg_p &lt;- dg_p %&gt;% mutate(Tissue = tissue) %&gt;% relocate(Tissue) La nueva columna indica el nombre de los tejidos a los que corresponde cada muestra. El vector de caracteres tissue está incluido en los datos que descargamos al comienzo de este capítulo. table(tissue) ## tissue ## cerebellum colon endometrium hippocampus kidney liver ## 38 34 15 31 39 26 ## placenta ## 6 Finalmente podemos realizar una gráfica de dispersión. En este caso solo utilizaré los valores de los componentes principales 1 y 2. ggplot(dg_p, aes(PC1, PC2, color = Tissue)) + geom_point(size = 2) + xlab(&quot;PC1 (19.5%)&quot;) + ylab(&quot;PC2 (12.3%)&quot;) + ggtitle(&quot;PCA en datos de expresión utilizando svd()&quot;) + theme_classic() A pesar de que ambos componentes solo suman el 32% de la variación total en los datos, es posible observar un par de agrupaciones. 5.4.3 Vectors propios y pesos Los vectores propios y pesos se pueden tomar directamente de la matriz V obtenida mediante SVD. dg_v[1:5, 1:5] ## [,1] [,2] [,3] [,4] [,5] ## [1,] 0.005355861 -0.001897947 -0.011063323 0.014768565 -0.0019776715 ## [2,] -0.005875013 -0.008384209 -0.005091850 -0.007090049 -0.0063618747 ## [3,] -0.006460185 0.005781902 0.003631691 0.001776129 0.0008765484 ## [4,] -0.001326232 0.002608896 -0.003894066 0.004410702 0.0210359224 ## [5,] 0.003255499 0.002272892 0.006577115 0.008688153 -0.0042652424 En este caso cada columna corresponde a un componente principal. Ya que tenemos un gran número de variables, realizar una gráfica de dispersión con los pesos de los primeros dos componentes no dejaría mucho en claro. Lo que sí podemos hacer es ordenar los valores absolutos de los pesos de mayor a menor para tener una idea de los genes con mayor contribución en ambos componentes. # Seleccionar los vectores propios correspondiente a PC 1 y 2 pesos_pc12 &lt;- dg_v[, 1:2] %&gt;% as.data.frame() %&gt;% rename(PC1 = &quot;V1&quot;, PC2 = &quot;V2&quot;) # Asignar los nombres de cada renglón a su respectivo gen rownames(pesos_pc12) &lt;- colnames(datos_genes) # Valores absolutos y ordenamiento de mayor a menor para PC1 pesos_pc1 &lt;- pesos_pc12 %&gt;% select(PC1) %&gt;% mutate(PC1 = abs(PC1)) %&gt;% arrange(desc(PC1)) De esta forma, se pueden observar los primeros diez genes con mayor peso en PC1. head(pesos_pc1, n = 10) ## PC1 ## 212215_at 0.01455813 ## 208861_s_at 0.01452472 ## 203855_at 0.01440628 ## 212928_at 0.01435030 ## 213049_at 0.01433674 ## 203607_at 0.01431453 ## 219137_s_at 0.01430930 ## 212982_at 0.01430322 ## 213505_s_at 0.01430079 ## 212765_at 0.01427708 Un procedimiento similar se puede seguir para PC2. pesos_pc2 &lt;- pesos_pc12 %&gt;% select(PC2) %&gt;% mutate(PC2 = abs(PC2)) %&gt;% arrange(desc(PC2)) head(pesos_pc2, n = 10) ## PC2 ## 201180_s_at 0.01730462 ## 202232_s_at 0.01696674 ## 203138_at 0.01688099 ## 202591_s_at 0.01658997 ## 217768_at 0.01649049 ## 205986_at 0.01648961 ## 200833_s_at 0.01647016 ## 201351_s_at 0.01644723 ## 201359_at 0.01643935 ## 200902_at 0.01641770 5.5 Análisis de componente principales con la función prcomp() Una manera mucho más directa de realizar PCA es con la función prcomp(). Esta función utiliza SVD y no requerimos de instalar ningún paquete adicional ya que se incluye en las funciones base de R. Al utilizar esta función tampoco necesitamos hacer operaciones o trasformaciones directas sobre los datos originales. dg_prcomp &lt;- prcomp(datos_genes, scale. = TRUE) El argumento scale. = TRUE especifica que los datos se dividan entre las desviaciones estándar, lo cual siempre es recomendable. 5.5.1 Gráfica de barras para la varianza de cada componente principal Para hacer una gráfica con los porcentajes de variación de cada componente principal, utilizamos los valores en sdev contenidos en dg_prcomp. # Valores propios vp_prcomp &lt;- dg_prcomp$sdev^2 / (nrow(datos_genes) - 1) # Porcentaje de variación para cada componente principal per_pc &lt;- data.frame(vp_prcomp) %&gt;% mutate( per_var = vp_prcomp * 100 / sum(vp_prcomp), pc = map_chr( 1:nrow(dg_prcomp$x), .f = function(x) paste0(&quot;PC&quot;, as.character(x)) ) ) # Gráfica con los porcentajes de variación de los primeros 15 componentes ggplot(per_pc[1:15,], aes(reorder(pc, -per_var), per_var)) + geom_col() + xlab(&quot;Componente principal&quot;) + ylab(&quot;Porcentaje de variación (%)&quot;) + theme_classic() Esta gráfica es la misma que la obtenida al realizar PCA con svd(). 5.5.2 Representación de los datos en dos dimensiones Las proyecciones de nuestros datos están en el objeto x dentro de dg_prcomp. # Proyecciones dg_p &lt;- dg_prcomp$x %&gt;% as.data.frame() %&gt;% mutate(Tissue = tissue) %&gt;% relocate(Tissue) # Gráfica de dispersión de los primeros dos componente ggplot(dg_p, aes(PC1, PC2, color = Tissue)) + geom_point(size = 2) + xlab(&quot;PC1 (33%)&quot;) + ylab(&quot;PC2 (14%)&quot;) + ggtitle(&quot;PCA en datos de expresión utilizando prcomp()&quot;) + theme_classic() Está gráfica también es la misma que al realizar el PCA mediante svd(). 5.5.3 Vectores propios y pesos Los vectores propios y pesos se pueden tomar directamente de rotation dentro de dg_prcomp. dg_prcomp$rotation[1:5, 1:5] ## PC1 PC2 PC3 PC4 PC5 ## 1007_s_at 0.005355861 -0.001897947 -0.011063323 0.014768565 -0.0019776715 ## 1053_at -0.005875013 -0.008384209 -0.005091850 -0.007090049 -0.0063618747 ## 117_at -0.006460185 0.005781902 0.003631691 0.001776129 0.0008765484 ## 121_at -0.001326232 0.002608896 -0.003894066 0.004410702 0.0210359224 ## 1255_g_at 0.003255499 0.002272892 0.006577115 0.008688153 -0.0042652424 En este caso no es necesario cambiar nombres o asignarlos. pesos_pc1 &lt;- dg_prcomp$rotation %&gt;% as.data.frame() %&gt;% select(PC1) %&gt;% mutate(PC1 = abs(PC1)) %&gt;% arrange(desc(PC1)) head(pesos_pc1, n = 10) ## PC1 ## 212215_at 0.01455813 ## 208861_s_at 0.01452472 ## 203855_at 0.01440628 ## 212928_at 0.01435030 ## 213049_at 0.01433674 ## 203607_at 0.01431453 ## 219137_s_at 0.01430930 ## 212982_at 0.01430322 ## 213505_s_at 0.01430079 ## 212765_at 0.01427708 Esta tabla también es la misma a la que obtuvimos con svd(). La obtención de la tabla para PC2 se deja como ejercicio para el lector. "],["análisis-de-componentes-principales-aplicado-a-metabolómica.html", " 6 Análisis de componentes principales aplicado a metabolómica 6.1 Datos 6.2 Reordenamiento de los datos 6.3 Análisis de componentes principales con prcomp()", " 6 Análisis de componentes principales aplicado a metabolómica Es hora de poner en práctica todo lo que hemos abordado en los capítulos anteriores. 6.1 Datos Los datos en este capítulo fueron generados aleatoriamente a partir de las medias y desviaciones estándar publicadas en Time-course metabolic profiling in Arabidopsis thaliana cell cultures after salt stress treatment. Si el lector tiene curiosidad puede consultar el archivo “simulacion_datos_met.R” en la carpeta “codigoR” situada en el repositorio de este manual. Los datos también se encuentran en el mismo repositorio dentro la carpeta datos. Los datos corresponden a un experimento con plantas de Arabidopsis thalania, de la cuales se realizaron cultivos celulares que fueron sometidos a distintos tiempos de estrés salino. El estrés salino es un factor importante que limita el crecimiento de las plantas y en el estudio estaban interesados en cómo cambia el nivel de algunos metabolitos primarios en estas condiciones. Como dato extra, la metabolómica es el estudio de las moléculas producidas por metabolismo celular, ya sea de plantas, bacterias, animales, etc. Las primeras diez columnas de nuestros datos se ven de la siguiente forma. library(dplyr) datos_met &lt;- readr::read_csv(&quot;datos/datos_met_completos.csv&quot;) head(datos_met, n = 10) ## # A tibble: 10 × 4 ## Metabolito Tiempo Muestra CR ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 aspartic acid 0.5 1 0.728 ## 2 aspartic acid 0.5 2 0.839 ## 3 aspartic acid 0.5 3 0.707 ## 4 aspartic acid 1 1 0.746 ## 5 aspartic acid 1 2 0.894 ## 6 aspartic acid 1 3 0.686 ## 7 aspartic acid 2 1 0.772 ## 8 aspartic acid 2 2 0.762 ## 9 aspartic acid 2 3 0.783 ## 10 aspartic acid 4 1 1.16 Es estos datos, CR signific “Cantidad Relativa” mientras que Muestra se refiere a las repeticiones por tratamiento o tiempo, que en cada caso fueron tres. 6.2 Reordenamiento de los datos Los datos como se muestran en la sección anterior están registrados de la manera usual y necesaria para hacer análisis de varianza o regresión. Pero en el caso de PCA necesitamos reordenarlos de tal manera que en las columnas se especifique cada variable (la cantidad relativa de cada metabolito) y en los renglones los pares muestra-tiempo. library(tidyr) dt_r &lt;- datos_met %&gt;% mutate(Muestra_Tiempo = paste0(Muestra, &quot;-&quot;, Tiempo)) %&gt;% pivot_wider( names_from = Muestra_Tiempo, values_from = CR, -Tiempo:-Muestra ) met_cr &lt;- t(dt_r[, -1]) colnames(met_cr) &lt;- dt_r$Metabolito met_cr[1:10, 1:5] ## aspartic acid asparagine serine glycine alanine ## 1-0.5 0.728 1.100 0.776 0.907 0.956 ## 2-0.5 0.839 0.978 0.789 0.908 0.960 ## 3-0.5 0.707 1.170 0.819 0.908 0.935 ## 1-1 0.746 1.000 0.828 0.857 0.829 ## 2-1 0.894 0.957 0.861 0.851 0.811 ## 3-1 0.686 0.953 0.796 0.858 0.844 ## 1-2 0.772 1.200 0.927 0.958 0.848 ## 2-2 0.762 1.290 0.829 0.970 0.790 ## 3-2 0.783 1.410 1.030 0.961 0.884 ## 1-4 1.160 1.020 0.899 0.826 0.937 6.3 Análisis de componentes principales con prcomp() Ahora que nuestros datos tienen la estructura adecuada podemos realizar el PCA con la función prcomp(). met_pca &lt;- prcomp(met_cr, scale. = TRUE) Como vimos en el capítulo anterior prcomp() devuelve un listado con las proyecciones de los datos, los pesos y valores propios de la matriz de covarianzas. También podemos utilizar la función summary() para desplegar un resumen donde solo se mostrará información sobre las varianzas y desviaciones estándar de cada componente principal. summary(met_pca) ## Importance of components: ## PC1 PC2 PC3 PC4 PC5 PC6 PC7 ## Standard deviation 4.3029 3.2325 1.95743 1.47536 1.46428 1.24333 1.06827 ## Proportion of Variance 0.4306 0.2430 0.08911 0.05062 0.04986 0.03595 0.02654 ## Cumulative Proportion 0.4306 0.6736 0.76269 0.81331 0.86318 0.89913 0.92567 ## PC8 PC9 PC10 PC11 PC12 PC13 PC14 ## Standard deviation 0.90340 0.7124 0.59916 0.52426 0.50298 0.44789 0.40629 ## Proportion of Variance 0.01898 0.0118 0.00835 0.00639 0.00588 0.00467 0.00384 ## Cumulative Proportion 0.94465 0.9565 0.96480 0.97119 0.97707 0.98174 0.98558 ## PC15 PC16 PC17 PC18 PC19 PC20 PC21 ## Standard deviation 0.37458 0.3471 0.31526 0.27399 0.24645 0.23404 0.18432 ## Proportion of Variance 0.00326 0.0028 0.00231 0.00175 0.00141 0.00127 0.00079 ## Cumulative Proportion 0.98884 0.9916 0.99395 0.99570 0.99711 0.99839 0.99918 ## PC22 PC23 PC24 ## Standard deviation 0.15186 0.11112 6.526e-16 ## Proportion of Variance 0.00054 0.00029 0.000e+00 ## Cumulative Proportion 0.99971 1.00000 1.000e+00 6.3.1 Gráfica de barras con la varianza de cada componente principal library(ggplot2) library(purrr) # Valores propios vp_prcomp &lt;- met_pca$sdev^2 / (nrow(datos_met) - 1) # Porcentaje de variación para cada componente principal per_pc &lt;- data.frame(vp_prcomp) %&gt;% mutate( per_var = vp_prcomp * 100 / sum(vp_prcomp), pc = map_chr( 1:nrow(met_pca$x), .f = function(x) paste0(&quot;PC&quot;, as.character(x)) ) ) # Gráfica con porcentajes para los primeros 15 componentes ggplot(per_pc[1:15,], aes(reorder(pc, -per_var), per_var)) + geom_col(fill = &quot;cornflowerblue&quot;, color = &quot;black&quot;) + xlab(&quot;Componente principal&quot;) + ylab(&quot;Porcentaje de variación (%)&quot;) + theme_classic() Los primeros dos componentes abarcan alrededor del 70% de la variación total en los datos, por lo que una gráfica de dispersión con estos dos componentes nos podría dar una idea de las semejanzas o diferencias entre los distintos tiempos de estrés salino. 6.3.2 Gráfica de dispesión con los primeros dos componentes Podemos darle un aspecto interactivo a nuestra gráfica de dispersión con la función ggploty() del paquete plotly. library(plotly) # Porcentajes de variación de los primeros dos componentes per_var_pc12 &lt;- round(per_pc$per_var[1:2], 1) # Tiempos tiempo &lt;- as.factor(substr(rownames(met_pca$x), start = 3, stop =5)) levels(tiempo) &lt;- c(&quot;0.5&quot;, &quot;1&quot;, &quot;2&quot;, &quot;4&quot;, &quot;12&quot;, &quot;24&quot;, &quot;48&quot;, &quot;72&quot;) # Proyecciones met_p &lt;- met_pca$x %&gt;% as.data.frame() %&gt;% mutate(Tiempo = tiempo) %&gt;% relocate(Tiempo) # Gráfica de dispersión de los primeros dos componente pc12 &lt;- ggplot(met_p, aes(PC1, PC2, color = Tiempo)) + geom_point(size = 2) + xlab(paste0(&quot;PC1 &quot;, &quot;(&quot;, per_var_pc12[1], &quot;%)&quot;)) + ylab(paste0(&quot;PC2 &quot;, &quot;(&quot;, per_var_pc12[2], &quot;%)&quot;)) + theme_classic() + scale_color_brewer(palette = &quot;Dark2&quot;) ggplotly(pc12) Podemos observar una clara diferenciación entre los distintos tiempos de estrés. En particular el tiempo de 72 horas a lo largo de PC1 y el tiempo de 12 horas a lo largo de PC2. 6.3.3 Peso o loadings de PC1 y PC2 Para determinar qué metabolitos son los que tienen una mayor peso o importancia en las agrupaciones o diferencias observadas en la gráfica anterior, podemos realizar una gráfica de paletas (lollipop chart) con los pesos absolutos de cada metabolito (ordenados de mayor a menor). Los pesos se encuentran en rotation dentro de los resultados devueltos por prcomp(). Primero observemos los referente al primer componente principal. # Obtener los pesos correspondientes de PC1 pesos_pc1 &lt;- met_pca$rotation %&gt;% as.data.frame() %&gt;% select(PC1) %&gt;% mutate(Metabolito = row.names(met_pca$rotation)) %&gt;% mutate(PC1 = round(abs(PC1), 3)) %&gt;% rename(Peso = PC1) # Gráfica de paletas con pesos de mayor a menor lolch_ppc1 &lt;- pesos_pc1 %&gt;% ggplot(aes(x = Peso, y = reorder(Metabolito, Peso))) + geom_segment(aes(x = 0, xend = Peso, yend = Metabolito), color = &quot;black&quot;) + geom_point(col = &quot;black&quot;, pch = 21, bg = &quot;#EE3B3B&quot;) + xlab(&quot;Peso&quot;) + ylab(&quot;Metabolito&quot;) + ggtitle(&quot;Pesos o loadigns en PC1&quot;) + theme_classic() # Aspecto interactivo con ggplotly() ggplotly(lolch_ppc1) Para los pesos del segundo componente principal seguimos el mismo procedimiento. # Obtener los pesos correspondientes de PC1 pesos_pc2 &lt;- met_pca$rotation %&gt;% as.data.frame() %&gt;% select(PC2) %&gt;% mutate(Metabolito = row.names(met_pca$rotation)) %&gt;% mutate(PC2 = round(abs(PC2), 3)) %&gt;% rename(Peso = PC2) # Gráfica de paleta con pesos de mayor a menor lolch_ppc2 &lt;- pesos_pc2 %&gt;% ggplot(aes(x = Peso, y = reorder(Metabolito, Peso))) + geom_segment(aes(x = 0, xend = Peso, yend = Metabolito), color = &quot;black&quot;) + geom_point(col = &quot;black&quot;, pch = 21, bg = &quot;#EE3B3B&quot;) + xlab(&quot;Peso&quot;) + ylab(&quot;Metabolito&quot;) + ggtitle(&quot;Pesos o loadigns en PC2&quot;) + theme_classic() # Aspecto interactivo con ggplotly() ggplotly(lolch_ppc2) ¿Cómo varían las cantidades de estos compuestos respecto al tiempo de estrés salino? Un primer paso podría ser realizar análisis de una sola variable en los metabolitos de mayor importancia (ANOVA y comparaciones múltiples). ¿Qué significa que algún metabolito tenga una mayor importancia o relevancia? La respuesta a esta pregunta dependerá del marco teórico de la investigación, tal vez algunos de estos compuestos tengan un papel central en el control del metabolismo ante situaciones de estrés. "],["una-shiny-app-para-automatizar-el-análisis-de-componentes-principales.html", " 7 Una Shiny App para automatizar el análisis de componentes principales 7.1 ¿Qué es PCA Maker? 7.2 Instalar y usar 7.3 Ejemplo", " 7 Una Shiny App para automatizar el análisis de componentes principales En este capítulo veremos como descargar y utilizar una Shiny App para llevar a cabo PCA. Esto puede considerarse una manera de automatizar el análisis para que no sea necesario escribir todo el código cada vez que lo necesitemos. Por cierto, la aplicación esta hecha por el autor de este manual y todo el código es de libre uso y acceso, por lo que, si tienes experiencia con Shiny, puedes adaptar, modificar o mejorar la aplicación. 7.1 ¿Qué es PCA Maker? PCA Maker es una aplicación Shiny que permite hacer y mostrar análisis de componentes principales a partir de datos en formato CSV o TSV. 7.2 Instalar y usar En R o R Studio, tienes dos opciones para utilizar PCA Maker. 7.2.1 Descargar de GitHub Simplemente descarga la aplicación de GitHub, da clic en el siguiente enlace: PCA-Maker. Una vez en la carpeta de principal de PCA Maker, da doble clic en el archivo PCA-Maker.Rproj: En la consola de R escribe devtools:load_all(\".\") (necesitas instalar el paquete devtools) y después PCAMaker(). También puedes correr el código en app.R, el cual está en la carpeta principal de la aplicación. 7.2.2 Como un paquete Instala la aplicación como un paquete con el código devtools::install_github(\"ciencia-libre/PCA-Maker\") y posteriormente escribe library(PCAMaker) y PCAMaker(). Al escribir PCAMaker() en la consola y presionar Enter se desplegará la aplicación y podrás navegar libremente por cada pestaña. Nota que la aplicación muestra un ejemplo pre-cargado. Este ejemplo corresponde a los datos de muestras de vino del capítulo tres. 7.3 Ejemplo Para este ejemplo utilizaremos lo datos de metabolómica del capítulo cinco, pero con una pequeña diferencia en su estructura. Dichos archivos están en la carpeta data del directorio principal de la aplicación. 7.3.1 Datos Los datos están en los archivos Met_quantities_data.csv y Met_time.csv. Para utilizar PCA Maker necesitamos dos archivos, uno con los datos de las respuestas o variables medidas (Met_quantities_data.csv): Y otro archivo donde especifiquemos las distintas categorías, estados, tratamientos o cualquier variable explicativa asociada a nuestras mediciones (Met_time.csv): Es importante asegurarnos que ambos conjuntos de datos tengan el mismo número de renglones. 7.3.2 Subir o cargar los datos Para subir o cargar nuestros datos, solo hay que hacer clic en el panel izquierdo en los botones Browse.... Esto abrirá una ventana que nos permitirá buscar y cargar nuestros datos. Una vez que carguemos los datos, el panel derecho desplegará un resumen de las variables respuesta: Y al final se mostrará una tabla de frecuencias de nuestra variable explicativa: 7.3.3 Personalizar el análisis En esta pestaña podemos escoger si las respuestas estarán centradas respecto a su media y divididas entre su desviación estándar. Ambas operaciones son opcionales, pero siempre recomendables en PCA. En el panel de la derecha se desplegará un resumen del PCA, donde básicamente solo se muestra información referente a la variación de cada componente principal. 7.3.4 Porcentajes de variación En el panel izquierdo de esta pestaña podemos personalizar el número de componentes en la gráfica de barras. También podemos descargar la figura en distintos formatos y los datos de porcentajes de variación en un archivo CSV. En el panel derecho podremos observar la gráfica de barras con los porcentajes de variación. 7.3.5 Proyecciones En el panel izquierdo de esta pestaña podremos seleccionar los componentes principales que incluiremos en la gráfica de dispersión y también podremos descargar la figura y los datos en formato CSV. En el panel derecho veremos la gráfica de dispersión con los componentes que seleccionamos. Los colores corresponden a cada nivel de la variable explicativa, y el porcentaje de variación de cada componente principal se indica entre paréntesis. 7.3.6 Pesos De forma similar, en el panel izquierdo podremos seleccionar los componentes principales en la gráfica y descargar los loadings en formato CSV. En el panel derecho veremos la gráfica de dispersión con los pesos. Hay que resaltar que entre más variables respuesta tengamos la gráfica será más difícil de interpretar por el gran número de puntos y nombres traslapados. 7.3.7 Bi-plot La gráfica en el panel derecho de esta pestaña conjunta las dos gráficas previas y también, entre más variables respuesta tengamos, puede que no sea una buena alternativa de visualización. El panel izquierdo también permite seleccionar los componentes y descargar la figura en distintos formatos. "],["referencias.html", " 8 Referencias", " 8 Referencias A continuación se listan referencias útiles para profundizar en los temas abordados en este manual. También se incluyen textos que pueden servir como punto de partida para el aprendizaje de R y algunas de sus herramientas. Ana María Kozak (2007) Nociones de geometría analítica y álgebra lineal. McGraw-Hill Interamericana. Bro, R. and Smilde, A.K. (2014) ‘Principal component analysis’, Anal. Methods, 6(9), pp. 2812–2831. Disponible en: https://doi.org/10.1039/C3AY41907J. Garrett Grolemund and Hadley Wickham (2016) R para ciencia de datos. O’Reilly Media, Inc. Disponible ent: https://es.r4ds.hadley.nz/. Hadley Wickham (2021) Mastering Shiny. O’Reilly Media, Inc. Disponible ent: https://mastering-shiny.org/. Kim, J.K. et al. (2007) ‘Time-course metabolic profiling in Arabidopsis thaliana cell cultures after salt stress treatment.’, Journal of experimental botany, 58(3), pp. 415–424. Disponible en: https://doi.org/10.1093/jxb/erl216. Sheldon Axler (2010) Linear Algebra Done Right. Springer US. Wickham, H. et al. (2019) ‘Welcome to the Tidyverse’, Journal of Open Source Software, 4(43), p. 1686. Disponible en: https://doi.org/10.21105/joss.01686. Yihui Xie (2017) bookdown Authoring Books and Technical Documents with R Markdown. Chapman and Hall/CRC. Disponible en: https://bookdown.org/yihui/bookdown/. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
